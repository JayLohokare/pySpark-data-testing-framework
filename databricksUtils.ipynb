{"cells":[{"cell_type":"code","source":["from datetime import datetime\nfrom pyspark.sql.functions import col, sum, avg"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["\n### 0. GLOBAL VARIABLES\n# Many dynamic-SQL functions use these lists to reference \n\n# The '28 data elements' that can receive scores for external corrosion risk\nscore_cols = [\"CP_Score\", \"CIS_Readings_Score\", \"Joint_Coating_Type_Score\", \"Joint_Coating_Install_Date_Score\", \"Joint_Coating_Condition_Score\", \"Mainline_Coating_Type_Score\", \"Mainline_Coating_Condition_SME_Score\",\n  \"Mainline_Coating_Install_Date_Score\", \"Active_Anomaly_Counts_Score\", \"ILI_Survey_Date_Score\", \"Stray_Current_Interference_Score\", \"Soil_Type_Score\", \"Casing_Inhibitor_Score\", \"Casing_Shorted_Score\",\n  \"Casing_Type_Score\", \"Land_Use_Score\", \"Exposed_Pipe_Score\", \"Pipe_Installation_Date_Score\", \"Pipe_Wall_Thickness_Score\", \"Maximum_Operating_Pressure_Score\", \"Pipe_Outside_Diameter_Score\",\n  \"Nominal_Material_Yield_Strength_Score\", \"Remaining_HalfLife_ECDA_Score\", \"EC_ECDA_Anomaly_Count_Score\", \"Year_ECDA_Score\", \"Hydrostatic_Retest_Date_Score\", \"EC_Reassessment_Interval_Score\",\n  \"External_Corrosion_Failure_History_Score\"]\n\n# The 28 names in the element weightage reference table corresponding to each of the above data elements\nweightage_cols = [\"cathodic_protection\", \"cathodic_protection\", \"joint_coating_type\", \"joint_coating_install_date\", \"joint_coating_condition\", \"mainline_coating\", \"mcc\",\n  \"age\", \"active_anomaly_counts\", \"ili_survey_date\", \"stray_current_interference\", \"soil_type\", \"casing_inhibitor\", \"casing_is_shorted\",\n  \"casing_type\", \"land_use\", \"exposed_pipe\", \"pipe_installation_date\", \"pipe_wall_thickness_base\", \"maximum_operating_pressure_base\", \"pipe_outside_diameter_base\",\n  \"nominal_material_yield_strength\", \"ecda_half_life\", \"ec_ecda_anomaly_counts\", \"year_of_ecda\", \"evaluation_date\", \"ec_reassessment_interval\",\n  \"external_corrosion_failure_history\"]\n\n# The 8 data groups into which the 28 data elements 'roll up'\ngroup_cols = [\"Assessment_Mitigation_Group_Score\", \"Stray_Current_Interference_Group_Score\", \"ECDA_Feature_Group_Score\", \"Joint_Coating_Group_Score\",\n  \"ILI_Group_Score\", \"Failure_History_Group_Score\", \"Baseline_Susceptibility_Group_Score\", \"Pipe_Specifications_Group_Score\"]\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["### 1. SETUP FUNCTIONS\n# get a reference to a string from the key vault\ndef get_secret(key, scope = \"Azure-KeyVault-Scope\"):\n  return dbutils.secrets.get(scope = scope,\n                             key = key)\n\n# configure the Databricks environment\ndef get_configs():\n  return {\"fs.azure.account.auth.type\": \"OAuth\",\n     \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n     \"fs.azure.account.oauth2.client.id\": get_secret(\"APR-TURING-ClientId\"),\n     \"fs.azure.account.oauth2.client.secret\": get_secret(\"APR-TURING-secret\"),\n     \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/\" + get_secret(\"APR-TURING-TenantId\") + \"/oauth2/token\",\n     \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\n# process an external parameter in teh notebook\ndef get_widget_argument(arg):\n  dbutils.widgets.text(arg, \"\", \"\")\n  dbutils.widgets.get(arg)\n  return getArgument(arg)\n\n# attach blob storage to the cluster\ndef mount_storage_with_configs(configs):\n  dbutils.fs.mount(\n  source = \"abfss://\" + get_secret(\"ADLS-GTM-connstr\") + \"/\",\n  mount_point = \"/mnt/turingdata\",\n  extra_configs = configs)\n\n# attach blob storage to the cluster\ndef mount_storage():\n  if any(mount.mountPoint == \"/mnt/turingdata\" for mount in dbutils.fs.mounts()):\n    return\n  configs = get_configs()\n  mount_storage_with_configs(configs)\n\n# detach blog storage from the cluster\ndef unmount_storage():\n  if any(mount.mountPoint == \"/mnt/turingdata\" for mount in dbutils.fs.mounts()):\n    dbutils.fs.unmount(\"/mnt/turingdata\")\n\n# use the 'slicestart' parameter to get the date of interest\ndef get_date_from_slicestart():\n  slicestart = get_widget_argument(\"SliceStart\")\n  date_time_obj = datetime.strptime(slicestart,'%m-%d-%Y')\n  date_yy = str(date_time_obj.year)\n  date_mm = str(date_time_obj.month)\n  date_dd = str(date_time_obj.day)\n\n  if len(date_dd) < 2:\n    date_dd = \"0\" + date_dd\n  if len(date_mm) < 2:\n    date_mm = \"0\" + date_mm\n  \n  return date_yy, date_mm, date_dd\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["### 2. EXTERNAL READ/WRITE FUNCTIONS\n# read in settings data as a Spark dataframe\ndef read_from_json(filename):\n  return spark.read.json(\"/mnt/turingdata/raw/solutions/turing/data/\" + filename) # finish this!!\n\n# read in columnar raw data as a Spark dataframe\ndef read_from_parquet(filename, source, year, month, day):\n  return spark.read.parquet(\"/mnt/turingdata/raw/global/pipeline_integrity/\" + source + \"/\" + filename + \"/\" + year + \"/\" + month + \"/\" + filename + \"_\" + day + \".parquet\")\n\ndef read_from_SQL(table_name):\n  jdbc_address = get_secret(\"SDB-SERVER-TURING-url\")\n  jdbc_username = get_secret(\"SDB-DATABRICKS-TURING-userstr\")\n  jdbc_password = get_secret(\"SDB-DATABRICKS-TURING-pwd\")\n\n  jdbc_url = \"jdbc:\" + jdbc_address + \";database=TURING;user=\" + jdbc_username + \";password=\" + jdbc_password + \";encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n\n  return spark.read.jdbc(url = jdbc_url, table = table_name)\n\ndef write_to_blob_storage(val, location):\n  fileName = location.split('/')[-1]\n  path = \"/\".join(location.split('/')[:-1])\n\n  if fileName not in [file.name for file in dbutils.fs.ls(path)]:\n    dbutils.fs.put(location, val)\n    \n  else:\n    print(\"INFO: File already exists, over-writing file \" + location)\n    try:\n      dbutils.fs.rm(location)\n      dbutils.fs.put(location, val)\n    except:\n      print(\"ERROR: Something went wrong while writing file, the existing file may have been deleted.\")\n\ndef write_df_to_blob_storage(df, location):\n  import csv\n  from io import StringIO\n  def row2csv(row):\n      buffer = StringIO()\n      writer = csv.writer(buffer)\n      writer.writerow([s for s in row])\n      buffer.seek(0)\n      return buffer.read().strip()\n\n  outputString = df.rdd.map(row2csv).coalesce(1)\n\n  val = \",\".join(schemaPeople.columns)\n  val += \"\\n\"\n  val += outputString.reduce(lambda x,y: \"\\n\".join([x,y]))\n  \n  write_to_blob_storage(val, location)\n\n# write a Spark dataframe to a SQL server table\ndef write_to_SQL(df, table_name, write_mode = \"overwrite\"):\n  jdbc_address = get_secret(\"SDB-SERVER-TURING-url\")\n  jdbc_username = get_secret(\"SDB-DATABRICKS-TURING-userstr\")\n  jdbc_password = get_secret(\"SDB-DATABRICKS-TURING-pwd\")\n\n  jdbc_url = \"jdbc:\" + jdbc_address + \";database=TURING;user=\" + jdbc_username + \";password=\" + jdbc_password + \";encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n\n  df.write.option(\"truncate\", \"true\").jdbc(url = jdbc_url, table = table_name, mode = \"overwrite\")\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["### 3. LOOKUP FUNCTIONS\n# get the value of an attribute of the ColorGuide table for a specific color\ndef get_color_attribute(table, attribute, color_category):\n  return spark.sql(\"SELECT \" + attribute + \" FROM \" + table + \" WHERE color_category = '\" + str(color_category) + \"'\").collect()[0].__getitem__(attribute)\n\n# get a flag indicating whether a specific data element has been implemented for an issue\ndef get_implemented(table, issue, element):\n  return (spark.sql(\"SELECT * FROM \" + table)).select(issue + \"_implemented\")\\\n    .filter((col(\"element\") == element))\\\n    .collect()[0].__getitem__(issue + \"_implemented\")\n\n# increase the weight of a category containing out-of-scope data elements\ndef get_multiplier(table, issue, element):\n  return (spark.sql(\"SELECT * FROM \" + table)).select(issue + \"_multiplier\")\\\n    .filter((col(\"group\") == element))\\\n    .collect()[0].__getitem__(issue + \"_multiplier\")\n\n# get a flag indicating whether a specific data element is in scope for an issue\ndef get_scope(table, issue, element):\n  return (spark.sql(\"SELECT * FROM \" + table)).select(issue +\"_scope\")\\\n    .filter((col(\"element\") == element))\\\n    .collect()[0].__getitem__(issue + \"_scope\")\n\n# get the relative weight of a data element\ndef get_weight(table, element):\n  return (spark.sql(\"SELECT * FROM \" + table)).select(\"weightage\")\\\n    .filter((col(\"element\") == element))\\\n    .collect()[0].__getitem__(\"weightage\")\n\n# get the relative weight of a data group\ndef get_group_weight(group, method):\n  return (spark.sql(\"SELECT * FROM groupWeightage\")).select(group)\\\n    .filter((col(\"method\") == method))\\\n    .collect()[0].__getitem__(group) "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["### 4. DASHBOARD FUNCTIONS\n# get the miles of pipe matching a certain color\ndef get_miles(table, color):\n  result = spark.sql(\"SELECT SUM(Length_Ft) / 5280 AS miles FROM \" + table + \" WHERE Color_Code = '\" + str(color) + \"'\").collect()[0].__getitem__(\"miles\")\n  if result == None:\n    return '0'\n  else:\n    return str(result)\n\n# get the percentage of pipe matching a certain color\ndef get_pct(table, color):\n  total = spark.sql(\"SELECT SUM(Length_Ft) AS total FROM \" + table).collect()[0].__getitem__(\"total\")\n  result = spark.sql(\"SELECT (SUM(Length_Ft) / \" + str(total) +  \") * 100 AS pct FROM \" + table + \" WHERE Color_Code = '\" + str(color) + \"'\").collect()[0].__getitem__(\"pct\")\n  if result == None:\n    return '0'\n  else:\n    return str(result)\n\n# get the kilometers of pipe matching a certain color\ndef get_kilometers(table, color):\n  result = spark.sql(\"SELECT SUM(Length_Ft) / 3280.84 AS kilometers FROM \" + table + \" WHERE Color_Code = '\" + str(color) + \"'\").collect()[0].__getitem__(\"kilometers\")\n  if result == None:\n    return '0'\n  else:\n    return str(result)\n\n# get the number of pipe units (routes, valve sections, etc.) containing at least one dynamic segment of a given color\ndef get_num_affected(table, color):\n  result = spark.sql(\"SELECT COUNT(*) AS num_affected FROM \" + table + \" WHERE Mileage_\" + color + \" > 0\").collect()[0].__getitem__(\"num_affected\")\n  if result == None:\n    return '0'\n  else:\n    return str(result)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["### 5. DYNAMIC SQL FUNCTIONS\n# SQL code to list every data element used for confidence calculations\ndef get_data_elements_sql(prefix = \"\", issue = None, multiplier = 100):\n  \n  sqlContext.cacheTable(\"elementWeightage\")\n  sql = \"\"\n  \n  for i in range(len(score_cols)):\n    if issue == None:\n      sql += prefix + score_cols[i] + \"\"\" * \"\"\" + str(multiplier) + \"\"\" AS \"\"\" + score_cols[i] + \"\"\", \"\"\"\n      \n    # mark whether a data element is in-scope-not-implemented or simply out-of-scope for a given issue\n    else:\n      sql += \"\"\"\n      CASE \n        WHEN \"\"\" + str(get_scope(\"elementweightage\", issue, weightage_cols[i])) + \"\"\" = 1 AND \"\"\" + str(get_implemented(\"elementweightage\", issue, weightage_cols[i])) + \"\"\" = 1\n          THEN \"\"\" + prefix + score_cols[i] + \"\"\" * \"\"\" + str(multiplier) + \"\"\"\n        WHEN \"\"\" + str(get_scope(\"elementweightage\", issue, weightage_cols[i])) + \"\"\" = 1 AND \"\"\" + str(get_implemented(\"elementweightage\", issue, weightage_cols[i])) + \"\"\" = 0\n          THEN -2\n        ELSE -1\n      END AS \"\"\" + score_cols[i] + \"\"\", \"\"\"   \n    \n  spark.catalog.uncacheTable(\"elementWeightage\") \n    \n  return sql[:-2]\n\n# list every data element used for confidence calculations according to the \"overall DC score\" rollup logic and rounding to a given precision\ndef get_data_elements_product_with_rounding_sql(table1_prefix, table2_prefix, table3_prefix, round_to, multiplier = 100):\n  sql = \"\"\n  \n  for col in score_cols:\n    sql += \"\"\"\n    CASE\n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NULL OR \"\"\" + table2_prefix + \"\"\"nullmarker = 1) AND (\"\"\" + table3_prefix + col + \"\"\" IS NULL OR \"\"\" + table3_prefix + \"\"\"nullmarker = 1)\n        THEN ROUND(\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + str(multiplier) + \"\"\", \"\"\" + round_to + \"\"\")\n        \n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NULL OR \"\"\" + table2_prefix + \"\"\"nullmarker = 1) AND (\"\"\" + table3_prefix + col + \"\"\" IS NOT NULL AND \"\"\" + table3_prefix + \"\"\"nullmarker = 0)\n        THEN ROUND((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\") * \"\"\" + str(multiplier) + \"\"\", \"\"\" + round_to + \"\"\")\n        \n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NOT NULL AND \"\"\" + table2_prefix + \"\"\"nullmarker = 0) AND (\"\"\" + table3_prefix + col + \"\"\" IS NULL OR \"\"\" + table3_prefix + \"\"\"nullmarker = 1)\n        THEN ROUND((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\") * \"\"\" + str(multiplier) + \"\"\", \"\"\" + round_to + \"\"\")\n        \n      ELSE ROUND(((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\") + \n        (\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\") - \n        (\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\")) * \"\"\" + str(multiplier) + \"\"\", \"\"\" + round_to + \"\"\")\n    END AS \"\"\" + col + \"\"\", \"\"\"\n\n  return sql[:-2]\n\n# list every data group used for confidence calculations\ndef get_data_groups_sql(prefix = \"\", multiplier = 100):\n  sql = \"\"\n  \n  for col in group_cols:\n      sql += col + \"\"\" * \"\"\" + str(multiplier) + \"\"\" AS \"\"\" + col + \"\"\", \"\"\"\n    \n  return sql[:-2]\n\n# list every data group used for confidence calculations according to the \"overall DC score\" rollup logic and rounding to a given precision\ndef get_data_groups_product_with_rounding_sql(table1_prefix, table2_prefix, table3_prefix, round_to, multiplier = 100):\n  sql = \"\"\n  \n  for col in group_cols:\n    sql += \"\"\"\n    CASE\n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NULL OR \"\"\" + table2_prefix + \"\"\"nullmarker = 1) AND (\"\"\" + table3_prefix + col + \"\"\" IS NULL OR \"\"\" + table3_prefix + \"\"\"nullmarker = 1)\n        THEN ROUND(\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + str(multiplier) + \"\"\", 2)\n        \n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NULL OR \"\"\" + table2_prefix + \"\"\"nullmarker = 1) AND (\"\"\" + table3_prefix + col + \"\"\" IS NOT NULL AND \"\"\" + table3_prefix + \"\"\"nullmarker = 0)\n        THEN ROUND((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\") * \"\"\" + str(multiplier) + \"\"\", 2)\n        \n      WHEN (\"\"\" + table2_prefix + col + \"\"\" IS NOT NULL AND \"\"\" + table2_prefix + \"\"\"nullmarker = 0) AND (\"\"\" + table3_prefix + col + \"\"\" IS NULL OR \"\"\" + table3_prefix + \"\"\"nullmarker = 1)\n        THEN ROUND((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\") * \"\"\" + str(multiplier) + \"\"\", 2)\n        \n      ELSE ROUND(((\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\") + \n        (\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\") - \n        (\"\"\" + table1_prefix + col + \"\"\" * \"\"\" + table2_prefix + col + \"\"\" * \"\"\" + table3_prefix + col + \"\"\")) * \"\"\" + str(multiplier) + \"\"\", 2)\n    END AS \"\"\" + col + \"\"\", \"\"\"\n    \n  return sql[:-2]\n\n# roll up every data element used for confidence calculations\ndef get_dc_data_elements_rollup_sql(prefix = \"\"):\n  sql = \"\"\n  \n  for col in score_cols:\n      sql += get_rollup_sql(col, prefix) + \" AS \" + col + \", \"\n \n  # remove final comma\n  return sql[:-2]\n\n# roll up every data group used for confidence calculations\ndef get_dc_data_groups_rollup_sql(prefix = \"\"):\n  sql = \"\"\n  \n  for col in group_cols:\n      sql += get_rollup_sql(col, prefix) + \" AS \" + col + \", \"\n \n  # remove final comma\n  return sql[:-2]\n\n# calculate an overall confidence score by getting relative weights by method and gorup\ndef get_all_group_weight_sql(prefix, table_type):\n  categories = [\"assessment_mitigation\", \"stray_current_interference\", \"ecda_feature\", \"joint_coating\", \"ili\",\n                \"failure_history\", \"baseline_susceptibility\", \"pipe_specifications\"]\n  \n  sql = \"\"\n  \n  for cat in categories:\n    sql += get_group_weight_sql(prefix, cat) + \" + \"\n    \n  # remove final comma\n  return sql[:-2]\n\n# SQL code to find the weight of a data group based on relative amounts of each calculation method\ndef get_group_weight_sql(prefix, group_prefix):\n  return \"((\" + prefix + group_prefix + \"_group_score * method.1_pct * \"\"\" + str(get_group_weight(group_prefix + \"_score\", 1)) + \") + (\" + prefix + group_prefix + \"_group_score * method.2_pct * \"\"\" + str(get_group_weight(group_prefix + \"_score\", 2)) + \"))\"\n\n# SQL code to assign a special marker to a table's row indicating that none of its in-scope data elements have been implemented\ndef get_null_marker_sql(issue):\n  implemented_cols = []\n  sql = \"CASE WHEN \"\n  \n  for i in range(len(weightage_cols)):\n    if get_scope(\"ElementWeightage\", issue, weightage_cols[i]) == 1:\n      implemented_cols.append(score_cols[i])\n  \n  for col in implemented_cols:\n    sql += col + \"\"\" IS NULL AND \"\"\"\n    \n  # remove final 'and'\n  sql = sql[:-4] + \" THEN 1 ELSE 0 END\"\n  \n  return sql\n\n# SQL code to roll up granular values weighted by their length\ndef get_rollup_sql(column_name, prefix = \"\"):\n  return \"SUM((\" + column_name + \") * \" + prefix + \"Length_Ft) / SUM(\" + prefix + \"Length_Ft) \"\n\n# SQL code to flag every element as in- or out-of-scope for an issue\ndef get_scope_sql(issue):\n  sql = \"\"\n  \n  for i in range(len(score_cols)):\n    sql += \"CASE WHEN \" + str(get_scope(\"ElementWeightage\", issue, weightage_cols[i])) + \" = 1 THEN COALESCE(\" + score_cols[i] + \", 0) ELSE NULL END AS \" + score_cols[i] + \", \"\n    \n  # remove final comma\n  return sql[:-2]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["### 6. SQL EXECUTION FUNCTIONS\n# add columns to a supplamentary table that will enable it to join with route-level data\ndef add_route_information_to_table(base_table, series_join_column, result_table):\n  spark.sql(\"\"\"\n    SELECT\n      x.*,\n      l.lineloopid AS Route_Section,\n      l.routedescription AS Route, \n      p.OperatingUnitCode AS Company_Code\n    FROM \n      \"\"\" + base_table + \"\"\" AS x\n      INNER JOIN StationSeries AS ss ON ss.series_id = x.\"\"\" + series_join_column + \"\"\"\n      INNER JOIN LineLoop AS l ON l.lineloopid = ss.LineLoopId \n      INNER JOIN PipelineSystem AS p on p.pipelineid = l.pipelineid\n  \"\"\").createOrReplaceTempView(result_table)\n\n# add columns to two supplamentary tables that will enable them to join with route-level data\ndef add_route_information_to_two_tables(base_table, second_table, series_join_column, first_join_column, second_join_column, result_table):\n  spark.sql(\"\"\"\n    SELECT\n      x.*,\n      y.*,\n      l.lineloopid AS Route_Section,\n      l.routedescription AS Route, \n      p.OperatingUnitCode AS Company_Code\n    FROM \n      \"\"\" + base_table + \"\"\" AS x\n      INNER JOIN \"\"\" + second_table + \"\"\" AS y ON x.\"\"\" + first_join_column + \"\"\" = y.\"\"\" + second_join_column + \"\"\"\n      INNER JOIN StationSeries AS ss ON ss.series_id = x.\"\"\" + series_join_column + \"\"\"\n      INNER JOIN LineLoop AS l ON l.lineloopid = ss.LineLoopId \n      INNER JOIN PipelineSystem AS p on p.pipelineid = l.pipelineid\n  \"\"\").createOrReplaceTempView(result_table)\n\n# assign color codes to a table based on risk and data confidence scores\ndef assign_color_codes(table_name, conf_column, risk_column):\n  return spark.sql(\"\"\"\n    SELECT\n      *,\n      CASE \n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'red')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'red')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'red')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'red')) + \"\"\" \n          THEN '\"\"\" + str(get_color_attribute(\"ColorGuide\", \"color_code\", 'red')) + \"\"\"'\n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'maroon')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'maroon')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'maroon')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'maroon')) + \"\"\" \n          THEN '\"\"\" + str(get_color_attribute(\"ColorGuide\", \"color_code\", 'maroon')) + \"\"\"'\n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'blue')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'blue')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'blue')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'blue')) + \"\"\" \n          THEN '\"\"\" + str(get_color_attribute(\"ColorGuide\", \"color_code\", 'blue')) + \"\"\"'\n        ELSE '\"\"\" + str(get_color_attribute(\"ColorGuide\", \"color_code\", 'gray')) + \"\"\"'\n      END AS Color_Code \n    FROM \"\"\" + table_name)\n\n# rol up by route section, weighted by length\ndef get_straight_avg_by_route(input_table, result_table, issue):\n  spark.sql(\"\"\"\n    SELECT\n      Company_Code,\n      Route,\n      AVG(\"\"\" + issue + \"\"\"_score) AS \"\"\" + issue + \"\"\"_Score\n    FROM\n      \"\"\" + input_table + \"\"\"\n    GROUP BY Company_Code, Route\n  \"\"\").createOrReplaceTempView(result_table)  \n  \n# roll up by route section, unweighted\ndef get_weighted_avg_by_route(input_table, result_table, issue):\n  spark.sql(\"\"\"\n    SELECT\n      Company_Code,\n      Route,\n      SUM(length * \"\"\" + issue + \"\"\"_score) / SUM(length) AS \"\"\" + issue + \"\"\"_Score\n    FROM\n      \"\"\" + input_table + \"\"\"\n    GROUP BY Company_Code, Route\n  \"\"\").createOrReplaceTempView(result_table)\n\n# roll up data elements into data groups for a given issue\ndef execute_full_group_rollup_sql(input_table,  result_table, issue):\n  spark.sql(\"\"\"\n  SELECT\n    *,\n    \n    ((GREATEST(hydrostatic_retest_date_score, Year_ECDA_Score) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"evaluation_date\")) + \"\"\") + \n    (COALESCE(ec_reassessment_interval_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"ec_reassessment_interval\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'assessment mitigation')) + \"\"\"\n      AS assessment_mitigation_group_score,\n      \n    (COALESCE(stray_current_interference_score, 0) *\n      \"\"\" + str(get_weight(\"ElementWeightage\", \"stray_current_interference\")) + \"\"\")\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'stray current interference')) + \"\"\" AS stray_current_interference_group_score,\n    \n    ((COALESCE(Remaining_HalfLife_ECDA_Score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"ecda_half_life\")) + \"\"\") + \n      (COALESCE(ec_ecda_anomaly_count_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"ec_ecda_anomaly_counts\")) + \"\"\") + \n      (COALESCE(Year_ECDA_Score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"year_of_ecda\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'ecda')) + \"\"\" AS ecda_feature_group_score,\n      \n    ((COALESCE(joint_coating_type_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"joint_coating_type\")) + \"\"\") + \n      (COALESCE(joint_coating_install_date_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"joint_coating_install_date\")) + \"\"\") + \n      (COALESCE(joint_coating_condition_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"joint_coating_condition\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'joint coating')) + \"\"\" AS joint_coating_group_score,\n     \n    ((COALESCE(active_anomaly_counts_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"active_anomaly_counts\")) + \"\"\") +\n      (COALESCE(ili_survey_date_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"ili_survey_date\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'ili'))  + \"\"\" AS ili_group_score,\n      \n    (COALESCE(external_corrosion_failure_history_score, 0) *\n      \"\"\" + str(get_weight(\"ElementWeightage\", \"external_corrosion_failure_history\")) + \"\"\")\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'failure history')) + \"\"\" AS failure_history_group_score,\n      \n    ((GREATEST(cp_score, cis_readings_score) *\n      \"\"\" + str(get_weight(\"ElementWeightage\", \"cathodic_protection\")) + \"\"\") +\n      (GREATEST(COALESCE(mainline_coating_install_date_score, 0), COALESCE(pipe_installation_date_score, 0)) *\n      \"\"\" + str(get_weight(\"ElementWeightage\", \"age\")) + \"\"\") + \n      (COALESCE(mainline_coating_type_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"mainline_coating\")) + \"\"\") +\n      (COALESCE(mainline_coating_condition_SME_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"mcc\")) + \"\"\") +\n      (COALESCE(pipe_wall_thickness_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"pipe_wall_thickness_base\")) + \"\"\") +\n      (COALESCE(maximum_operating_pressure_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"maximum_operating_pressure_base\")) + \"\"\") +\n      (COALESCE(pipe_outside_diameter_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"pipe_outside_diameter_base\")) + \"\"\") +\n      (COALESCE(nominal_material_yield_strength_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"nominal_material_yield_strength\")) + \"\"\") +\n      (COALESCE(soil_type_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"soil_type\")) + \"\"\") + \n      (COALESCE(casing_inhibitor_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"casing_inhibitor\")) + \"\"\") +\n      (COALESCE(Casing_Shorted_Score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"casing_is_shorted\")) + \"\"\") + \n      (COALESCE(casing_type_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"casing_type\")) + \"\"\") + \n      (COALESCE(land_use_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"land_use\")) + \"\"\") + \n      (COALESCE(exposed_pipe_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"exposed_pipe\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'baseline susceptibility')) + \"\"\" AS baseline_susceptibility_group_score,\n       \n    ((COALESCE(pipe_installation_date_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"pipe_installation_date\")) + \"\"\") + \n      (COALESCE(pipe_wall_thickness_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"pipe_wall_thickness_spec\")) + \"\"\") +\n      (COALESCE(maximum_operating_pressure_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"maximum_operating_pressure_spec\")) + \"\"\") + \n      (COALESCE(pipe_outside_diameter_score, 0) * \n      \"\"\" + str(get_weight(\"ElementWeightage\", \"pipe_outside_diameter_spec\")) + \"\"\"))\n      * \"\"\" + str(get_multiplier('ElementWeightage', issue, 'pipe specification')) + \"\"\" AS pipe_specifications_group_score\n      \n  FROM \n  \"\"\" + input_table).createOrReplaceTempView(result_table)\n\n# roll up data groups into an overall score for a given issue\ndef execute_full_score_rollup_sql(input_table, result_table, issue):\n  spark.sql(\"\"\"\n  SELECT\n    x.Company_Code AS Company_Code,\n    x.Route AS Route,\n    (\n      \"\"\" + get_all_group_weight_sql(\"x.\", issue) + \"\"\"\n    )  AS Route_\"\"\" + issue + \"\"\"_Score, \n    \"\"\" + get_data_groups_sql(\"x.\", 1) + \"\"\",\n    \"\"\" + get_data_elements_sql(\"x.\", None, 1) + \"\"\",\n    x.nullmarker\n  FROM \n    \"\"\" + input_table + \"\"\" x\n    INNER JOIN (\n      SELECT \n        Company_Code,\n        Route,\n        Data_Confidence_Calculation_Method AS 2_Pct,\n        1 - Data_Confidence_Calculation_Method AS 1_Pct\n      FROM\n         Availability_Route_Score\n    ) AS Method ON Method.Company_Code = x.Company_Code AND\n        Method.Route = x.Route\n\"\"\").createOrReplaceTempView(result_table)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["### 7. HELPER FUNCTIONS\ndef see(query, n = 200):\n  spark.sql(query).show(n, truncate = False)\n  \ndef list_to_string(x):\n  output = \"\"\n  \n  for elem in x:\n    output += elem + \",\"\n  \n  return output[:-1]\n\ndef drop_temp_tables(list_of_tables):\n  for table in list_of_tables:\n    spark.catalog.dropTempView(table)\n\n# see how many different values a column takes on\ndef distinct_values(table, column):\n  return spark.sql(\"SELECT COUNT(DISTINCT \" + column + \") AS distinct_\" + column + \"s FROM \" + table).show()\n\ndef sum_col(df, col):\n  return df.select(sum(col)).collect()[0][0]\n  \ndef avg_col(df, col):\n  return df.select(avg(col)).collect()[0][0]\n  \ndef get_weighted_avg(table, col):\n  return spark.sql(\"SELECT SUM(\" + col + \" * Length_Ft) / SUM(Length_Ft) AS weighted_avg FROM \" + table).collect()[0].__getitem__(\"weighted_avg\")\n\ndef get_max_date(table, col):\n  return spark.sql(\"SELECT MAX(\" + col + \") AS \" + col + \" FROM \" + table).collect()[0][0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["### DEPRICATED FUNCTION\n# Design changes rendered these obsolete, but the thing about design changes is that they are often reversed\n\ndef get_dc_data_groups_product(table1_prefix, table2_prefix):\n  sql = \"\"\n  \n  for col in group_cols:\n    sql += \"COALESCE(\" + table1_prefix + col + \", 1) * COALESCE(\" + table2_prefix + col + \", 1) / 100 AS \" + col + \", \"\n    \n  return sql[:-2]\n\ndef get_dc_data_elements_product(table1_prefix, table2_prefix):\n  sql = \"\"\n  \n  for col in score_cols:\n    sql += \"COALESCE(\" + table1_prefix + col + \", 1) * COALESCE(\" + table2_prefix + col + \", 1) AS \" + col + \", \"\n    \n  return sql[:-2]\n\n# assign color numbers to a table based on risk and data confidence scores\ndef assign_color_numbers(table_name, conf_column, risk_column):\n  return spark.sql(\"\"\"\n    SELECT\n      *,\n      CASE \n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'red')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'red')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'red')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'red')) + \"\"\" \n          THEN 3\n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'maroon')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'maroon')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'maroon')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'maroon')) + \"\"\" \n          THEN 2\n        WHEN \"\"\" + conf_column + \"\"\" \"\"\" + str(get_color_attribute(\"ColorGuide\", \"dc_operator\", 'blue')) + str(get_color_attribute(\"ColorGuide\", \"dc_threshold\", 'blue')) + \"\"\" AND \n          \"\"\" + risk_column + str(get_color_attribute(\"ColorGuide\", \"risk_operator\", 'blue')) + str(get_color_attribute(\"ColorGuide\", \"risk_threshold\", 'blue')) + \"\"\" \n          THEN 0\n        ELSE 1\n      END AS Color_Number\n    FROM \"\"\" + table_name)"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"gtm_utils","notebookId":1874594338652422},"nbformat":4,"nbformat_minor":0}
